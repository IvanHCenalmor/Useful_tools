{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"ray[data]\" torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with the hyperparameter scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown # Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# Define model\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # In this example, we don't change the model architecture\n",
    "        # due to simplicity.\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
    "        self.fc = nn.Linear(192, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = x.view(-1, 192)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "# Setting up a Tuner for a Training Run with Tune\n",
    "    \n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "def train_mnist(config):\n",
    "    # Data Setup\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=True, download=True, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = ConvNet()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "    for i in range(10):\n",
    "        train_func(model, optimizer, train_loader)\n",
    "        acc = test_func(model, test_loader)\n",
    "\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            checkpoint = None\n",
    "            if (i + 1) % 5 == 0:\n",
    "                # This saves the model to the trial directory\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(temp_checkpoint_dir, \"model.pth\")\n",
    "                )\n",
    "                checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "            # Send the current training result back to Tune\n",
    "            train.report({\"mean_accuracy\": acc}, checkpoint=checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown # Setting up a Tuner for a Training Run with Tune\n",
    "\n",
    "# Let’s run one trial by calling Tuner.fit and randomly sample from a uniform distribution for learning rate and momentum.\n",
    "\n",
    "search_space = {\n",
    "    \"lr\": tune.sample_from(lambda spec: 10 ** (-10 * np.random.rand())),\n",
    "    \"momentum\": tune.uniform(0.1, 0.9),\n",
    "}\n",
    "\n",
    "# Uncomment this to enable distributed execution\n",
    "# `ray.init(address=\"auto\")`\n",
    "\n",
    "# Download the dataset first\n",
    "datasets.MNIST(\"~/data\", train=True, download=True)\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    train_mnist,\n",
    "    param_space=search_space,\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "#Tuner.fit returns an ResultGrid object. You can use this to plot the performance of this trial.\n",
    "\n",
    "dfs = {result.path: result.metrics_dataframe for result in results}\n",
    "[d.mean_accuracy.plot() for d in dfs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown # Early Stopping with Adaptive Successive Halving (ASHAScheduler)\n",
    "\n",
    "'''\n",
    "Let’s integrate early stopping into our optimization process. \n",
    "Let’s use ASHA, a scalable algorithm for principled early stopping.\n",
    "\n",
    "On a high level, ASHA terminates trials that are less promising and allocates more time and resources to more promising trials. \n",
    "As our optimization process becomes more efficient, we can afford to increase the search space by 5x, by adjusting the parameter num_samples.\n",
    "\n",
    "ASHA is implemented in Tune as a “Trial Scheduler”. \n",
    "These Trial Schedulers can early terminate bad trials, pause trials, clone trials, and alter hyperparameters of a running trial.\n",
    "See the TrialScheduler documentation for more details of available schedulers and library integrations.\n",
    "'''\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    train_mnist,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=20,\n",
    "        scheduler=ASHAScheduler(metric=\"mean_accuracy\", mode=\"max\"),\n",
    "    ),\n",
    "    param_space=search_space,\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "# Obtain a trial dataframe from all run trials of this `tune.run` call.\n",
    "dfs = {result.path: result.metrics_dataframe for result in results}\n",
    "\n",
    "# Plot by epoch\n",
    "ax = None  # This plots everything on the same plot\n",
    "for d in dfs.values():\n",
    "    ax = d.mean_accuracy.plot(ax=ax, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown # Using Search Algorithms in Tune\n",
    "\n",
    "'''\n",
    "In addition to TrialSchedulers, you can further optimize your hyperparameters by \n",
    "using an intelligent search technique like Bayesian Optimization. To do this, \n",
    "you can use a Tune Search Algorithm. Search Algorithms leverage optimization \n",
    "algorithms to intelligently navigate the given hyperparameter space.\n",
    "\n",
    "Note that each library has a specific way of defining the search space.\n",
    "'''\n",
    "\n",
    "from hyperopt import hp\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.loguniform(\"lr\", -10, -1),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.1, 0.9),\n",
    "}\n",
    "\n",
    "hyperopt_search = HyperOptSearch(space, metric=\"mean_accuracy\", mode=\"max\")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    train_mnist,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=10,\n",
    "        search_alg=hyperopt_search,\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "# To enable GPUs, use this instead:\n",
    "# analysis = tune.run(\n",
    "#     train_mnist, config=search_space, resources_per_trial={'gpu': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown # Evaluating Your Model after Tuning\n",
    "\n",
    "'''\n",
    "You can evaluate best trained model using the ExperimentAnalysis object to retrieve the best model:\n",
    "'''\n",
    "\n",
    "best_result = results.get_best_result(\"mean_accuracy\", mode=\"max\")\n",
    "with best_result.checkpoint.as_directory() as checkpoint_dir:\n",
    "    state_dict = torch.load(os.path.join(checkpoint_dir, \"model.pth\"))\n",
    "\n",
    "model = ConvNet()\n",
    "model.load_state_dict(state_dict)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
